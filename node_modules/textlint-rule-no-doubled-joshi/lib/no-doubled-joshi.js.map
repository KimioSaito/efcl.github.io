{"version":3,"sources":["../src/no-doubled-joshi.js"],"names":[],"mappings":";AACA,YAAY,CAAC;;;;;;kBA8DE,UAAU,OAAO,EAAgB;QAAd,OAAO,yDAAG,EAAE;;AAC1C,QAAM,MAAM,GAAG,wBA9DX,UAAU,CA8DgB,OAAO,CAAC;;AAAC,AAEvC,QAAM,WAAW,GAAG,OAAO,CAAC,YAAY,IAAI,cAAc,CAAC,YAAY,CAAC;AACxE,QAAM,QAAQ,GAAG,OAAO,CAAC,MAAM,IAAI,cAAc,CAAC,MAAM,CAAC;QAClD,MAAM,GAAkC,OAAO,CAA/C,MAAM;QAAE,MAAM,GAA0B,OAAO,CAAvC,MAAM;QAAE,SAAS,GAAe,OAAO,CAA/B,SAAS;QAAE,SAAS,GAAI,OAAO,CAApB,SAAS;;AAC3C,+BACK,MAAM,CAAC,SAAS,YAAE,IAAI,EAAC;AACpB,YAAI,MAAM,CAAC,WAAW,CAAC,IAAI,EAAE,CAAC,MAAM,CAAC,IAAI,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,CAAC,UAAU,EAAE,MAAM,CAAC,QAAQ,CAAC,CAAC,EAAE;AAC3F,mBAAO;SACV;AACD,YAAM,MAAM,GAAG,mCAAiB,IAAI,CAAC,CAAC;AACtC,YAAM,IAAI,GAAG,MAAM,CAAC,QAAQ,EAAE,CAAC;AAC/B,YAAM,cAAc,GAAG,SAAjB,cAAc,CAAG,IAAI,EAAI;AAC3B,mBAAO,IAAI,CAAC,IAAI,KAAK,kBAzEb,MAAM,CAyEsB,QAAQ,CAAC;SAChD,CAAC;AACF,YAAI,SAAS,GAAG,gCAAe,IAAI,EAAE;AACjC,sBAAU,EAAE,WAAW;SAC1B,CAAC,CAAC,MAAM,CAAC,cAAc,CAAC,CAAC;AAC1B,eAAO,eA/EX,YAAY,GA+Ea,CAAC,IAAI,CAAC,UAAA,SAAS,EAAI;AACpC,gBAAM,aAAa,GAAG,SAAhB,aAAa,CAAI,QAAQ,EAAK;AAChC,oBAAI,MAAM,GAAG,SAAS,CAAC,mBAAmB,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC;AACzD,oBAAI,eAAe,GAAG,MAAM,CAAC,MAAM,CAAC,UAAA,KAAK,EAAI;AACzC,wBAAI,QAAQ,EAAE;AACV,+BAAO,gBAhF/B,SAAS,EAgFgC,KAAK,CAAC,CAAC;qBAC3B;;;;AAAA,AAID,2BAAO,gBArF3B,SAAS,EAqF4B,KAAK,CAAC,IAAI,gBArFpC,SAAS,EAqFqC,KAAK,CAAC,CAAC;iBAC/C,CAAC,CAAC;AACH,oBAAI,uBAAuB,GAAG,mBAAmB,CAAC,eAAe,CAAC;;;;;;;;;AAAC,AAUnE,sBAAM,CAAC,IAAI,CAAC,uBAAuB,CAAC,CAAC,OAAO,CAAC,UAAA,GAAG,EAAI;AAChD,wBAAM,MAAM,GAAG,uBAAuB,CAAC,GAAG,CAAC,CAAC;AAC5C,wBAAM,SAAS,GAAG,gBAlGpB,uBAAuB,EAkGqB,GAAG,CAAC;;AAAC,AAE/C,wBAAI,CAAC,QAAQ,EAAE;AACX,4BAAI,kBAAkB,CAAC,MAAM,CAAC,EAAE;AAC5B,mCAAO;yBACV;qBACJ;AACD,wBAAI,MAAM,CAAC,MAAM,IAAI,CAAC,EAAE;AACpB;AAAO,qBACV;;;AAAA,AAGD,wBAAI,OAAO,GAAG,SAAV,OAAO,CAAI,IAAI,EAAE,OAAO,EAAK;AAC7B,4BAAM,aAAa,GAAG,eAAe,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC;AACpD,4BAAM,aAAa,GAAG,eAAe,CAAC,OAAO,CAAC,OAAO,CAAC;;AAAC,AAEvD,4BAAM,eAAe,GAAG,aAAa,GAAG,aAAa,CAAC;AACtD,4BAAI,eAAe,IAAI,WAAW,EAAE;AAChC,gCAAM,gBAAgB,GAAG,MAAM,CAAC,mBAAmB,CAAC;AAChD,oCAAI,EAAE,QAAQ,CAAC,GAAG,CAAC,KAAK,CAAC,IAAI;AAC7B,sCAAM,EAAE,QAAQ,CAAC,GAAG,CAAC,KAAK,CAAC,MAAM,IAAI,OAAO,CAAC,aAAa,GAAG,CAAC,CAAA,AAAC;6BAClE,CAAC;;AAAC,AAEH,gCAAM,OAAO,GAAG;AACZ,oCAAI,EAAE,gBAAgB,CAAC,IAAI,GAAG,CAAC;;;AAG/B,sCAAM,EAAE,gBAAgB,CAAC,MAAM;6BAClC,CAAC;AACF,kCAAM,CAAC,IAAI,EAAE,IAAI,SAAS,yBAAsB,SAAS,mBAAe,OAAO,CAAC,CAAC,CAAC;yBACrF;AACD,+BAAO,OAAO,CAAC;qBAClB,CAAC;AACF,0BAAM,CAAC,MAAM,CAAC,OAAO,CAAC,CAAC;iBAC1B,CAAC,CAAC;aACN,CAAC;AACF,qBAAS,CAAC,OAAO,CAAC,aAAa,CAAC,CAAC;SACpC,CAAC,CAAC;KACN,EACJ;CACJ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA/HD,SAAS,mBAAmB,CAAC,MAAM,EAAE;;AAEjC,WAAO,MAAM,CAAC,MAAM,aAdpB,SAAS,CAcsB,CAAC,MAAM,CAAC,UAAC,MAAM,EAAE,KAAK,EAAK;;AAEtD,YAAM,QAAQ,GAAG,gBAfrB,gBAAgB,EAesB,KAAK,CAAC,CAAC;AACzC,YAAI,CAAC,MAAM,CAAC,QAAQ,CAAC,EAAE;AACnB,kBAAM,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC;SACzB;AACD,cAAM,CAAC,QAAQ,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;AAC7B,eAAO,MAAM,CAAC;KACjB,EAAE,EAAE,CAAC,CAAC;CACV;AACD,SAAS,kBAAkB,CAAC,MAAM,EAAE;AAChC,QAAI,KAAK,GAAG,MAAM,CAAC,CAAC,CAAC;;AAAC,AAEtB,QAAI,KAAK,CAAC,YAAY,KAAK,KAAK,EAAE;AAC9B,eAAO,IAAI,CAAC;KACf;;AAAA,AAED,QAAI,KAAK,CAAC,YAAY,KAAK,KAAK,IAAI,KAAK,CAAC,YAAY,KAAK,GAAG,EAAE;AAC5D,eAAO,IAAI,CAAC;KACf;;AAAA,AAED,QAAI,KAAK,CAAC,YAAY,KAAK,MAAM,IAAI,KAAK,CAAC,YAAY,KAAK,GAAG,EAAE;AAC7D,eAAO,IAAI,CAAC;KACf;AACD,WAAO,KAAK,CAAC;CAChB;;;;AAAA,AAID,IAAM,cAAc,GAAG;AACnB,gBAAY,EAAE,CAAC;AACf,UAAM,EAAE,KAAK;CAChB;;;;;;;;;;AAAC,AA6FD,CAAC","file":"no-doubled-joshi.js","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport {RuleHelper} from \"textlint-rule-helper\";\nimport {getTokenizer} from \"kuromojin\";\nimport splitSentences, {Syntax as SentenceSyntax} from \"sentence-splitter\";\nimport StringSource from \"textlint-util-to-string\";\nimport {\n    is助詞Token, is読点Token,\n    createKeyFromKey, restoreToSurfaceFromKey\n} from \"./token-utils\";\n/**\n * Create token map object\n * {\n *  \"で\": [token, token],\n *  \"の\": [token, token]\n * }\n * @param tokens\n * @returns {*}\n */\nfunction createSurfaceKeyMap(tokens) {\n    // 助詞のみを対象とする\n    return tokens.filter(is助詞Token).reduce((keyMap, token) => {\n        // \"は:助詞.係助詞\" : [token]\n        const tokenKey = createKeyFromKey(token);\n        if (!keyMap[tokenKey]) {\n            keyMap[tokenKey] = [];\n        }\n        keyMap[tokenKey].push(token);\n        return keyMap;\n    }, {});\n}\nfunction matchExceptionRule(tokens) {\n    let token = tokens[0];\n    // \"の\" の重なりは例外\n    if (token.pos_detail_1 === \"連体化\") {\n        return true;\n    }\n    // \"を\" の重なりは例外\n    if (token.pos_detail_1 === \"格助詞\" && token.surface_form === \"を\") {\n        return true;\n    }\n    // 接続助詞 \"て\" の重なりは例外\n    if (token.pos_detail_1 === \"接続助詞\" && token.surface_form === \"て\") {\n        return true;\n    }\n    return false;\n}\n/*\n    default options\n */\nconst defaultOptions = {\n    min_interval: 1,\n    strict: false\n};\n\n/*\n    1. Paragraph Node -> text\n    2. text -> sentences\n    3. tokenize sentence\n    4. report error if found word that match the rule.\n\n    TODO: need abstraction\n */\nexport default function (context, options = {}) {\n    const helper = new RuleHelper(context);\n    // 最低間隔値\n    const minInterval = options.min_interval || defaultOptions.min_interval;\n    const isStrict = options.strict || defaultOptions.strict;\n    const {Syntax, report, getSource, RuleError} = context;\n    return {\n        [Syntax.Paragraph](node){\n            if (helper.isChildNode(node, [Syntax.Link, Syntax.Image, Syntax.BlockQuote, Syntax.Emphasis])) {\n                return;\n            }\n            const source = new StringSource(node);\n            const text = source.toString();\n            const isSentenceNode = node => {\n                return node.type === SentenceSyntax.Sentence;\n            };\n            let sentences = splitSentences(text, {\n                charRegExp: /[。\\?\\!？！]/\n            }).filter(isSentenceNode);\n            return getTokenizer().then(tokenizer => {\n                const checkSentence = (sentence) => {\n                    let tokens = tokenizer.tokenizeForSentence(sentence.raw);\n                    let countableTokens = tokens.filter(token => {\n                        if (isStrict) {\n                            return is助詞Token(token);\n                        }\n                        // デフォルトでは、\"、\"を間隔値の距離としてカウントする\n                        // \"、\" があると助詞同士の距離が開くようにすることで、並列的な\"、\"の使い方を許容する目的\n                        // https://github.com/azu/textlint-rule-no-doubled-joshi/issues/2\n                        return is助詞Token(token) || is読点Token(token);\n                    });\n                    let joshiTokenSurfaceKeyMap = createSurfaceKeyMap(countableTokens);\n                    /*\n                    # Data Structure\n\n                        joshiTokens = [tokenA, tokenB, tokenC, tokenD, tokenE, tokenF]\n                        joshiTokenSurfaceKeyMap = {\n                            \"は:助詞.係助詞\": [tokenA, tokenC, tokenE],\n                            \"で:助詞.係助詞\": [tokenB, tokenD, tokenF]\n                        }\n                     */\n                    Object.keys(joshiTokenSurfaceKeyMap).forEach(key => {\n                        const tokens = joshiTokenSurfaceKeyMap[key];\n                        const joshiName = restoreToSurfaceFromKey(key);\n                        // strict mode ではない時例外を除去する\n                        if (!isStrict) {\n                            if (matchExceptionRule(tokens)) {\n                                return;\n                            }\n                        }\n                        if (tokens.length <= 1) {\n                            return;// no duplicated token\n                        }\n                        // if found differenceIndex less than\n                        // tokes are sorted ascending order\n                        var reduder = (prev, current) => {\n                            const startPosition = countableTokens.indexOf(prev);\n                            const otherPosition = countableTokens.indexOf(current);\n                            // 助詞token同士の距離が設定値以下ならエラーを報告する\n                            const differenceIndex = otherPosition - startPosition;\n                            if (differenceIndex <= minInterval) {\n                                const originalPosition = source.originalPositionFor({\n                                    line: sentence.loc.start.line,\n                                    column: sentence.loc.start.column + (current.word_position - 1)\n                                });\n                                // padding positionを計算する\n                                const padding = {\n                                    line: originalPosition.line - 1,\n                                    // matchLastToken.word_position start with 1\n                                    // this is padding column start with 0 (== -1)\n                                    column: originalPosition.column\n                                };\n                                report(node, new RuleError(`一文に二回以上利用されている助詞 \"${joshiName}\" がみつかりました。`, padding));\n                            }\n                            return current;\n                        };\n                        tokens.reduce(reduder);\n                    });\n                };\n                sentences.forEach(checkSentence);\n            });\n        }\n    }\n};\n"]}